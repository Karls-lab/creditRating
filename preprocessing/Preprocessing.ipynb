{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "import sys \n",
    "import os\n",
    "\n",
    "\"\"\"\n",
    "Generate SMOTE minority class by 10\n",
    "Down sample the majority class to the length of the minority class\n",
    "\"\"\"\n",
    "def downsample_majority_class(df, target_column, random_state=None):\n",
    "    df = df.drop(columns=['case_id'])\n",
    "    minority_class = df[df[target_column] == 1]\n",
    "    print(\"number of minority class: \", len(minority_class))\n",
    "\n",
    "    # Use SMOTE to upsample the minority class by x5 \n",
    "    count = len(minority_class) * 10\n",
    "    smote = SMOTE(sampling_strategy={1: count})\n",
    "    X = df.drop(columns=target_column)\n",
    "    y = df[target_column]\n",
    "    X, y = smote.fit_resample(X, y)\n",
    "    print(\"number of minority class after SMOTE: \", len(y[y == 1]))\n",
    "    print(\"number of majority class: \", len(y[y == 0]))\n",
    "\n",
    "    SMOTE_df = pd.concat([pd.DataFrame(X, columns=X.columns), pd.Series(y, name=target_column)], axis=1)\n",
    "    SMOTE_df_majority = SMOTE_df[SMOTE_df[target_column] == 0]\n",
    "    SMOTE_df_minority = SMOTE_df[SMOTE_df[target_column] == 1]\n",
    "    \n",
    "    # Down sample the majority class to match the number of minority class\n",
    "    down_sampled_majority = SMOTE_df_majority.sample(n=len(y[y == 1]), random_state=random_state)\n",
    "    balanced_df = pd.concat([down_sampled_majority, SMOTE_df_minority], axis=0)\n",
    "\n",
    "    return balanced_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "\"\"\" Declare categorial and numerical features. Filter df down to these features and \n",
    "    case_id and target \n",
    "\"\"\"\n",
    "def processMain(df, keep_col=None):\n",
    "    # Read in variables from features.csv, which contains the categorical and numerical features\n",
    "    f_df = pd.read_csv('features.csv')\n",
    "    cat_f = f_df[f_df['type'] == 'categorical']['name'].tolist()\n",
    "    num_f = f_df[f_df['type'] == 'numerical']['name'].tolist()\n",
    "    display(f\"categorical features: {cat_f}\")\n",
    "    display(f\"numerical features: {num_f}\")\n",
    "    col_to_keep = keep_col + cat_f + num_f\n",
    "    df = df[col_to_keep]\n",
    "    print(f\"df columns: {df.columns}\")\n",
    "    df.to_csv('test/TEST0.csv', index=False)\n",
    "\n",
    "    # For categorical variables, take the most recent value\n",
    "    for col in cat_f:\n",
    "        for case_id in df['case_id'].unique():\n",
    "            try: \n",
    "                common_val = df[df['case_id'] == case_id][col].value_counts().idxmax()\n",
    "            except ValueError: \n",
    "                common_val = 0\n",
    "            df.loc[df['case_id'] == case_id, col] = common_val\n",
    "\n",
    "    # For numerical variables, take the mean of the values\n",
    "    for col in num_f:\n",
    "        for case_id in df['case_id'].unique():\n",
    "            common_val = df[df['case_id'] == case_id][col].mean()\n",
    "            df.loc[df['case_id'] == case_id, col] = common_val\n",
    "\n",
    "    # round to 2 decimal places. Replace missing numerical values with the mean\n",
    "    df.loc[:, num_f] = df[num_f].round(2)\n",
    "    df.loc[:, num_f] = df[num_f].fillna(df[num_f].mean())\n",
    "\n",
    "    df.to_csv('test/TEST1.csv', index=False)\n",
    "\n",
    "    # Now for case_id's the data is aggregated, so now we can drop duplicates\n",
    "    df = df.drop_duplicates(subset='case_id')\n",
    "    df.to_csv('test/TEST2.csv', index=False)\n",
    "\n",
    "    # one-hot encode the categorical variables\n",
    "    df = pd.get_dummies(df, columns=cat_f, dtype=np.int8)\n",
    "\n",
    "    # standardize the continuous variables\n",
    "    scaler = StandardScaler()\n",
    "    df[num_f] = scaler.fit_transform(df[num_f]) \n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"categorical features: ['district_544M', 'mainoccupationinc_437A', 'cancelreason_3545846M']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\"numerical features: ['annuity_853A', 'credamount_590A']\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df columns: Index(['case_id', 'target', 'district_544M', 'mainoccupationinc_437A',\n",
      "       'cancelreason_3545846M', 'annuity_853A', 'credamount_590A'],\n",
      "      dtype='object')\n",
      "number of minority class:  195\n",
      "number of minority class after SMOTE:  1950\n",
      "number of majority class:  5538\n",
      "\n",
      "Preprocessed dataframe info:\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 3900 entries, 1130 to 7487\n",
      "Columns: 821 entries, annuity_853A to target\n",
      "dtypes: float64(2), int64(1), int8(818)\n",
      "memory usage: 3.2 MB\n",
      "target\n",
      "0    1950\n",
      "1    1950\n",
      "Name: count, dtype: int64\n",
      "len of columns: 821\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\"\"\" Display the number of cases with target 1 and 0 \"\"\"\n",
    "# df = df[df['target'] == 1]\n",
    "# df.to_csv(os.path.join(rootPath, 'data', 'processed', 'target_1.csv'), index=False)\n",
    "\n",
    "# for each df in the test folder, process the data \n",
    "# file_path = '../data/home-credit-credit-risk-model-stability/csv_files/test'\n",
    "# file_path = '../data/home-credit-credit-risk-model-stability/csv_files/train'\n",
    "# file_path = '../data/home-credit-credit-risk-model-stability/csv_files/train/train_applprev_1_0.csv'\n",
    "# df = pd.read_csv(file_path)\n",
    "\n",
    "df = pd.read_csv(os.path.join(\"../\", \"data\", \"mergedDatasets\", \"person&ApplprevTrain.csv\"))\n",
    "df = processMain(df, keep_col=['case_id', 'target'])\n",
    "balanced_df = downsample_majority_class(df, 'target')\n",
    "\n",
    "print(f\"\\nPreprocessed dataframe info:\\n\")\n",
    "balanced_df.info()\n",
    "print(balanced_df['target'].value_counts())\n",
    "balanced_df.to_csv(os.path.join('../', 'data', 'processed', 'final.csv'), index=False)\n",
    "print(f'len of columns: {len(balanced_df.columns)}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
